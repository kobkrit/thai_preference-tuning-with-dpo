{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiE8lbVmmzNn"
      },
      "source": [
        "# Direct Preference Optimization (DPO) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Train Post-Processing LLM\n",
        "\n",
        "## ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ Qwen3-0.6B (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ + Thinking Mode)\n",
        "\n",
        "## ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç\n",
        "1. [DPO ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?](#1-dpo-‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£)\n",
        "2. [‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Qwen3?](#2-‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ-qwen3)\n",
        "3. [‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°](#3-‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°)\n",
        "4. [‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Preference](#4-‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•-preference)\n",
        "5. [‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡πÅ‡∏•‡∏∞ DataLoader](#5-‡∏™‡∏£‡πâ‡∏≤‡∏á-dataset-‡πÅ‡∏•‡∏∞-dataloader)\n",
        "6. [‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì DPO Loss](#6-‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì-dpo-loss)\n",
        "7. [‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ DPO](#7-‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢-dpo)\n",
        "8. [‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå](#8-‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTqmyxIgmzNp"
      },
      "source": [
        "---\n",
        "## 1. DPO ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
        "\n",
        "**Direct Preference Optimization (DPO)** ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å LLM ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏≠‡∏ö (Preference) ‡∏Ç‡∏≠‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå\n",
        "\n",
        "### ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ DPO?\n",
        "\n",
        "‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏≥ **Instruction Finetuning** ‡πÅ‡∏•‡πâ‡∏ß ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡∏à‡∏∞:\n",
        "- ‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û\n",
        "- ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\n",
        "- ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ\n",
        "\n",
        "DPO ‡∏ä‡πà‡∏ß‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• (Align) ‡∏Å‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå\n",
        "\n",
        "### ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö RLHF vs DPO\n",
        "\n",
        "| RLHF | DPO |\n",
        "|------|-----|\n",
        "| ‡∏ï‡πâ‡∏≠‡∏á‡∏ù‡∏∂‡∏Å Reward Model ‡πÅ‡∏¢‡∏Å | ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Reward Model |\n",
        "| ‡πÉ‡∏ä‡πâ PPO (‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô) | ‡πÉ‡∏ä‡πâ Loss function ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á |\n",
        "| ‡∏ù‡∏∂‡∏Å‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ | ‡∏ù‡∏∂‡∏Å‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ |\n",
        "| ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏°‡∏≤‡∏Å | ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£ |\n",
        "\n",
        "### ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á DPO\n",
        "\n",
        "DPO ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö **‡∏Ñ‡∏π‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏≠‡∏ö (Preference Pairs)**:\n",
        "- **Chosen (‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å)**: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏î‡∏µ ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏£‡πâ‡∏≤‡∏á\n",
        "- **Rejected (‡∏ñ‡∏π‡∏Å‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò)**: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏î‡∏µ ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
        "\n",
        "‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏à‡∏∞:\n",
        "- **‡πÄ‡∏û‡∏¥‡πà‡∏°** ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á Chosen response\n",
        "- **‡∏•‡∏î** ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á Rejected response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltgNv1zemzNp"
      },
      "source": [
        "---\n",
        "## 2. ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Qwen3-0.6B?\n",
        "\n",
        "### ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á Qwen3-0.6B\n",
        "\n",
        "| Feature | Qwen3-0.6B |\n",
        "|---------|------------|\n",
        "| **‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢** | ‚úÖ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö (100+ ‡∏†‡∏≤‡∏©‡∏≤) |\n",
        "| **Thinking Mode** | ‚úÖ ‡∏°‡∏µ (`<think>` tags) |\n",
        "| **Parameters** | 600M (‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ) |\n",
        "| **Context Length** | 32,768 tokens |\n",
        "| **License** | Apache 2.0 (‡πÉ‡∏ä‡πâ‡πÄ‡∏ä‡∏¥‡∏á‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå‡πÑ‡∏î‡πâ) |\n",
        "\n",
        "### Qwen3 Thinking Mode\n",
        "\n",
        "Qwen3 ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö **Thinking Mode** ‡∏ã‡∏∂‡πà‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞:\n",
        "1. ‡∏Ñ‡∏¥‡∏î‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÉ‡∏ô `<think>...</think>` block ‡∏Å‡πà‡∏≠‡∏ô\n",
        "2. ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∂‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
        "\n",
        "```\n",
        "<think>\n",
        "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö...\n",
        "‡∏â‡∏±‡∏ô‡∏à‡∏∞‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏°‡∏∏‡∏°‡∏°‡∏≠‡∏á...\n",
        "</think>\n",
        "\n",
        "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô...\n",
        "```\n",
        "\n",
        "### ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DPO ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤\n",
        "\n",
        "Dataset ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏°‡∏µ `<think>` tags ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö Qwen3:\n",
        "- **Chosen**: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏û‡∏£‡πâ‡∏≠‡∏° reasoning ‚úÖ\n",
        "- **Rejected**: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© (‡∏ú‡∏¥‡∏î‡∏†‡∏≤‡∏©‡∏≤) ‚ùå"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWp5NOHYmzNp"
      },
      "source": [
        "---\n",
        "## 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°\n",
        "\n",
        "### ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
        "\n",
        "- **Python**: 3.10 - 3.12 (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ **Python 3.11**)\n",
        "- **Memory**: ~5 GB RAM/VRAM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DPO (2 ‡πÇ‡∏°‡πÄ‡∏î‡∏•)\n",
        "\n",
        "‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á libraries ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mSySXaDmzNp"
      },
      "outputs": [],
      "source": [
        "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Python version\n",
        "import sys\n",
        "print(f\"Python version: {sys.version}\")\n",
        "assert sys.version_info >= (3, 10) and sys.version_info < (3, 13), \\\n",
        "    \"‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Python 3.10 - 3.12 (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ 3.11)\"\n",
        "\n",
        "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á packages ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô (‡∏£‡∏±‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)\n",
        "# !pip install torch matplotlib tqdm safetensors huggingface_hub tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWqPP-1BmzNq"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import json\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö version\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"Using Apple Silicon GPU (MPS)\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BnL5XVqmzNq"
      },
      "outputs": [],
      "source": [
        "# Import Qwen3 Model ‡πÅ‡∏•‡∏∞ Tokenizer\n",
        "from qwen3 import (\n",
        "    Qwen3Model,\n",
        "    Qwen3Tokenizer,\n",
        "    QWEN_CONFIG_06_B,\n",
        "    load_weights_into_qwen3,\n",
        "    download_from_huggingface,\n",
        "    download_from_huggingface_from_snapshots\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Qwen3 modules imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Zadep9kmzNq"
      },
      "outputs": [],
      "source": [
        "# Qwen3-0.6B Configuration\n",
        "print(\"Qwen3-0.6B Configuration:\")\n",
        "print(\"=\" * 40)\n",
        "for k, v in QWEN_CONFIG_06_B.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbL2D38UmzNq"
      },
      "source": [
        "---\n",
        "## 4. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Preference\n",
        "\n",
        "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Preference ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢:\n",
        "- `instruction`: ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)\n",
        "- `input`: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° (optional)\n",
        "- `chosen`: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏î‡∏µ - ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡∏û‡∏£‡πâ‡∏≠‡∏° `<think>` reasoning\n",
        "- `rejected`: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏î‡∏µ - ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© (‡∏ú‡∏¥‡∏î‡∏†‡∏≤‡∏©‡∏≤)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qxKfOtNmzNr"
      },
      "outputs": [],
      "source": [
        "# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DPO ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
        "with open(\"dpo_thai_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt6ves3RmzNr"
      },
      "outputs": [],
      "source": [
        "# ‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "example = data[0]\n",
        "print(\"=\" * 60)\n",
        "print(\"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DPO:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nüìù Instruction:\\n{example['instruction'][:200]}...\")\n",
        "print(f\"\\n‚úÖ Chosen (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ + <think>):\\n{example['chosen'][:300]}...\")\n",
        "print(f\"\\n‚ùå Rejected (‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© - ‡∏ú‡∏¥‡∏î!):\\n{example['rejected'][:300]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePBaM1lBmzNr"
      },
      "source": [
        "### ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:\n",
        "- **Chosen**: ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô**‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢** ‡∏û‡∏£‡πâ‡∏≠‡∏° `<think>` reasoning ‚úÖ\n",
        "- **Rejected**: ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô**‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©** (‡∏ú‡∏¥‡∏î‡∏†‡∏≤‡∏©‡∏≤!) ‚ùå\n",
        "\n",
        "DPO ‡∏à‡∏∞‡∏™‡∏≠‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡πâ:\n",
        "1. ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ñ‡∏π‡∏Å‡∏ñ‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
        "2. ‡πÉ‡∏ä‡πâ reasoning ‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏≠‡∏ö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irFVVfkGmzNr"
      },
      "outputs": [],
      "source": [
        "# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 70% train, 15% val, 15% test\n",
        "n = len(data)\n",
        "train_size = int(0.7 * n)\n",
        "val_size = int(0.15 * n)\n",
        "\n",
        "train_data = data[:train_size]\n",
        "val_data = data[train_size:train_size + val_size]\n",
        "test_data = data[train_size + val_size:]\n",
        "\n",
        "print(f\"Train: {len(train_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\")\n",
        "print(f\"Validation: {len(val_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\")\n",
        "print(f\"Test: {len(test_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z1jYgVEmzNr"
      },
      "source": [
        "---\n",
        "## 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡πÅ‡∏•‡∏∞ DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAzgPyGzmzNr"
      },
      "outputs": [],
      "source": [
        "# ‡πÇ‡∏´‡∏•‡∏î Qwen3 Tokenizer\n",
        "REPO_ID = \"Qwen/Qwen3-0.6B\"\n",
        "LOCAL_DIR = \"qwen3_weights\"\n",
        "\n",
        "# Download tokenizer\n",
        "download_from_huggingface(\n",
        "    repo_id=REPO_ID,\n",
        "    filename=\"tokenizer.json\",\n",
        "    local_dir=LOCAL_DIR\n",
        ")\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á tokenizer\n",
        "tokenizer = Qwen3Tokenizer(\n",
        "    tokenizer_file_path=f\"{LOCAL_DIR}/tokenizer.json\",\n",
        "    repo_id=REPO_ID,\n",
        "    apply_chat_template=False,  # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ template ‡πÄ‡∏≠‡∏á\n",
        "    add_generation_prompt=False,\n",
        "    add_thinking=False\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Tokenizer loaded\")\n",
        "print(f\"Vocab size: {QWEN_CONFIG_06_B['vocab_size']:,}\")\n",
        "print(f\"PAD token ID: {tokenizer.pad_token_id}\")\n",
        "print(f\"EOS token ID: {tokenizer.eos_token_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqBlglsAmzNr"
      },
      "outputs": [],
      "source": [
        "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö tokenizer ‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
        "test_thai = \"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\"\n",
        "tokens = tokenizer.encode(test_thai, chat_wrapped=False)\n",
        "print(f\"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: {test_thai}\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô tokens: {len(tokens)}\")\n",
        "print(f\"Decoded: {tokenizer.decode(tokens)}\")\n",
        "\n",
        "# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©\n",
        "test_eng = \"Hello, this is a test\"\n",
        "tokens_eng = tokenizer.encode(test_eng, chat_wrapped=False)\n",
        "print(f\"\\nEnglish: {test_eng}\")\n",
        "print(f\"Tokens: {len(tokens_eng)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxJifAx3mzNr"
      },
      "outputs": [],
      "source": [
        "def format_prompt_qwen3(entry):\n",
        "    \"\"\"\n",
        "    ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Qwen3 chat format\n",
        "    \"\"\"\n",
        "    instruction = entry['instruction']\n",
        "    input_text = entry.get('input', '')\n",
        "\n",
        "    if input_text:\n",
        "        user_content = f\"{instruction}\\n\\n‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°: {input_text}\"\n",
        "    else:\n",
        "        user_content = instruction\n",
        "\n",
        "    # Qwen3 chat format\n",
        "    prompt = f\"<|im_start|>user\\n{user_content}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMxL2WAemzNr"
      },
      "outputs": [],
      "source": [
        "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö format\n",
        "example = data[0]\n",
        "formatted = format_prompt_qwen3(example)\n",
        "print(\"Formatted prompt:\")\n",
        "print(formatted[:300])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2VHBpa9mzNr"
      },
      "outputs": [],
      "source": [
        "class PreferenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DPO Training with Qwen3\n",
        "    \"\"\"\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "        # Pre-process\n",
        "        self.processed_data = []\n",
        "        for entry in data:\n",
        "            prompt = format_prompt_qwen3(entry)\n",
        "\n",
        "            # Full sequences (prompt + response + end token)\n",
        "            chosen_full = prompt + entry[\"chosen\"] + \"<|im_end|>\"\n",
        "            rejected_full = prompt + entry[\"rejected\"] + \"<|im_end|>\"\n",
        "\n",
        "            self.processed_data.append({\n",
        "                \"prompt\": prompt,\n",
        "                \"chosen\": chosen_full,\n",
        "                \"rejected\": rejected_full\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.processed_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.processed_data[idx]\n",
        "\n",
        "        # Tokenize\n",
        "        prompt_tokens = self.tokenizer.encode(item[\"prompt\"], chat_wrapped=False)\n",
        "        chosen_tokens = self.tokenizer.encode(item[\"chosen\"], chat_wrapped=False)\n",
        "        rejected_tokens = self.tokenizer.encode(item[\"rejected\"], chat_wrapped=False)\n",
        "\n",
        "        # Truncate\n",
        "        prompt_tokens = prompt_tokens[:self.max_length]\n",
        "        chosen_tokens = chosen_tokens[:self.max_length]\n",
        "        rejected_tokens = rejected_tokens[:self.max_length]\n",
        "\n",
        "        return {\n",
        "            \"prompt\": torch.tensor(prompt_tokens),\n",
        "            \"chosen\": torch.tensor(chosen_tokens),\n",
        "            \"rejected\": torch.tensor(rejected_tokens),\n",
        "            \"prompt_length\": len(prompt_tokens)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpNbInu1mzNr"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch, pad_token_id, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Collate function ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö padding batch\n",
        "    \"\"\"\n",
        "    max_chosen = max(len(item[\"chosen\"]) for item in batch)\n",
        "    max_rejected = max(len(item[\"rejected\"]) for item in batch)\n",
        "    max_prompt = max(len(item[\"prompt\"]) for item in batch)\n",
        "\n",
        "    chosen_list, rejected_list, prompt_list = [], [], []\n",
        "    chosen_mask_list, rejected_mask_list = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        # Pad chosen\n",
        "        chosen = item[\"chosen\"]\n",
        "        chosen_padded = F.pad(chosen, (0, max_chosen - len(chosen)), value=pad_token_id)\n",
        "        chosen_list.append(chosen_padded)\n",
        "\n",
        "        chosen_mask = torch.ones(len(chosen))\n",
        "        chosen_mask = F.pad(chosen_mask, (0, max_chosen - len(chosen)), value=0)\n",
        "        chosen_mask_list.append(chosen_mask)\n",
        "\n",
        "        # Pad rejected\n",
        "        rejected = item[\"rejected\"]\n",
        "        rejected_padded = F.pad(rejected, (0, max_rejected - len(rejected)), value=pad_token_id)\n",
        "        rejected_list.append(rejected_padded)\n",
        "\n",
        "        rejected_mask = torch.ones(len(rejected))\n",
        "        rejected_mask = F.pad(rejected_mask, (0, max_rejected - len(rejected)), value=0)\n",
        "        rejected_mask_list.append(rejected_mask)\n",
        "\n",
        "        # Pad prompt\n",
        "        prompt = item[\"prompt\"]\n",
        "        prompt_padded = F.pad(prompt, (0, max_prompt - len(prompt)), value=pad_token_id)\n",
        "        prompt_list.append(prompt_padded)\n",
        "\n",
        "    return {\n",
        "        \"prompt\": torch.stack(prompt_list).to(device),\n",
        "        \"chosen\": torch.stack(chosen_list).to(device),\n",
        "        \"rejected\": torch.stack(rejected_list).to(device),\n",
        "        \"chosen_mask\": torch.stack(chosen_mask_list).to(device),\n",
        "        \"rejected_mask\": torch.stack(rejected_mask_list).to(device)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3NmdnBJmzNs"
      },
      "outputs": [],
      "source": [
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset\n",
        "train_dataset = PreferenceDataset(train_data, tokenizer, max_length=512)\n",
        "val_dataset = PreferenceDataset(val_data, tokenizer, max_length=512)\n",
        "\n",
        "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
        "print(f\"Val dataset: {len(val_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdVoMG3cmzNs"
      },
      "outputs": [],
      "source": [
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader\n",
        "from functools import partial\n",
        "\n",
        "custom_collate = partial(collate_fn, pad_token_id=tokenizer.pad_token_id, device=device)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    collate_fn=custom_collate\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=False,\n",
        "    collate_fn=custom_collate\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gecSWwLZmzNs"
      },
      "outputs": [],
      "source": [
        "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö DataLoader\n",
        "batch = next(iter(train_loader))\n",
        "print(\"Batch keys:\", batch.keys())\n",
        "print(f\"Chosen shape: {batch['chosen'].shape}\")\n",
        "print(f\"Rejected shape: {batch['rejected'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNq5Q11amzNs"
      },
      "source": [
        "---\n",
        "## 6. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì DPO Loss\n",
        "\n",
        "### ‡∏™‡∏π‡∏ï‡∏£ DPO Loss\n",
        "\n",
        "$$\\mathcal{L}_{DPO} = -\\log \\sigma \\left( \\beta \\left( \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} \\right) \\right)$$\n",
        "\n",
        "‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà:\n",
        "- $\\pi_\\theta$: Policy model (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞ train)\n",
        "- $\\pi_{ref}$: Reference model (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô - frozen)\n",
        "- $y_w$: Chosen response (‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏î‡∏µ - ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)\n",
        "- $y_l$: Rejected response (‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏î‡∏µ - ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)\n",
        "- $\\beta$: Temperature parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7PSERpmmzNs"
      },
      "source": [
        "### ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ DPO Loss ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢\n",
        "\n",
        "---\n",
        "\n",
        "## ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏π‡πâ: ‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î 2 ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô!\n",
        "\n",
        "**DPO ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ RAM/VRAM ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£ fine-tune ‡∏õ‡∏Å‡∏ï‡∏¥ 2 ‡πÄ‡∏ó‡πà‡∏≤** ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• 2 ‡∏ï‡∏±‡∏ß‡πÉ‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô:\n",
        "\n",
        "![DPO Memory Requirement](https://github.com/kobkrit/thai_preference-tuning-with-dpo/blob/main/images/dpo_memory_requirement.png?raw=1)\n",
        "\n",
        "| ‡πÇ‡∏°‡πÄ‡∏î‡∏• | ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà | ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ | Memory |\n",
        "|-------|--------|-------|--------|\n",
        "| **Policy Model** (œÄ_Œ∏) | ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á train | Trainable (‡∏°‡∏µ gradient) | ~100% |\n",
        "| **Reference Model** (œÄ_ref) | ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á ‡∏Å‡πà‡∏≠‡∏ô train | Frozen (‡πÑ‡∏°‡πà‡∏°‡∏µ gradient) | ~100% |\n",
        "\n",
        "### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Memory\n",
        "\n",
        "```\n",
        "Qwen3-0.6B:\n",
        "- 1 ‡πÇ‡∏°‡πÄ‡∏î‡∏• ‚âà 1.2 GB (BF16)\n",
        "- DPO (2 ‡πÇ‡∏°‡πÄ‡∏î‡∏•) ‚âà 2.4 GB + gradients ‚âà 4-5 GB\n",
        "\n",
        "Qwen3-4B:\n",
        "- 1 ‡πÇ‡∏°‡πÄ‡∏î‡∏• ‚âà 8 GB (BF16)\n",
        "- DPO (2 ‡πÇ‡∏°‡πÄ‡∏î‡∏•) ‚âà 16 GB + gradients ‚âà 24-30 GB\n",
        "```\n",
        "\n",
        "**‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Reference Model?**\n",
        "- ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏à‡∏ô‡∏•‡∏∑‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÄ‡∏î‡∏¥‡∏°\n",
        "- ‡πÄ‡∏õ‡πá‡∏ô \"‡∏à‡∏∏‡∏î‡∏¢‡∏∂‡∏î\" ‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏´‡∏•‡∏∏‡∏î‡πÑ‡∏õ‡πÑ‡∏Å‡∏• (Regularization)\n",
        "\n",
        "---\n",
        "\n",
        "## Visualization: DPO Loss ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\n",
        "\n",
        "### Step 1: Forward Pass ‡∏ó‡∏±‡πâ‡∏á 2 ‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
        "\n",
        "![DPO Forward Pass](https://github.com/kobkrit/thai_preference-tuning-with-dpo/blob/main/images/dpo_forward_pass.png?raw=1)\n",
        "\n",
        "### Step 2: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Log Ratio\n",
        "\n",
        "![DPO Log Ratio](https://github.com/kobkrit/thai_preference-tuning-with-dpo/blob/main/images/dpo_log_ratio.png?raw=1)\n",
        "\n",
        "### Step 3: DPO Loss\n",
        "\n",
        "![DPO Loss Formula](https://github.com/kobkrit/thai_preference-tuning-with-dpo/blob/main/images/dpo_loss_formula.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVlh0HJymzNs"
      },
      "outputs": [],
      "source": [
        "# Visualization: DPO Loss ‡πÅ‡∏•‡∏∞ Sigmoid Function\n",
        "import numpy as np\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# 1. Sigmoid Function\n",
        "x = np.linspace(-5, 5, 100)\n",
        "sigmoid = 1 / (1 + np.exp(-x))\n",
        "axes[0].plot(x, sigmoid, 'b-', linewidth=2)\n",
        "axes[0].axhline(y=0.5, color='r', linestyle='--', alpha=0.5)\n",
        "axes[0].axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
        "axes[0].set_xlabel('x = Œ≤ √ó (chosen_ratio - rejected_ratio)', fontsize=10)\n",
        "axes[0].set_ylabel('œÉ(x)', fontsize=10)\n",
        "axes[0].set_title('Sigmoid Function\\nœÉ(x) = 1 / (1 + e^(-x))', fontsize=12)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. DPO Loss\n",
        "dpo_loss = -np.log(sigmoid + 1e-10)\n",
        "axes[1].plot(x, dpo_loss, 'g-', linewidth=2)\n",
        "axes[1].axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
        "axes[1].fill_between(x[x > 0], 0, dpo_loss[x > 0], alpha=0.3, color='green', label='Thai > Eng (‡∏î‡∏µ)')\n",
        "axes[1].fill_between(x[x < 0], 0, dpo_loss[x < 0], alpha=0.3, color='red', label='Thai < Eng (‡πÑ‡∏°‡πà‡∏î‡∏µ)')\n",
        "axes[1].set_xlabel('Œ≤ √ó (Thai_ratio - Eng_ratio)', fontsize=10)\n",
        "axes[1].set_ylabel('DPO Loss', fontsize=10)\n",
        "axes[1].set_title('DPO Loss Function\\nL = -log(œÉ(x))', fontsize=12)\n",
        "axes[1].legend(loc='upper right', fontsize=9)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].set_ylim(0, 6)\n",
        "\n",
        "# 3. Memory Usage\n",
        "methods = ['SFT\\n(1 model)', 'DPO\\n(2 models)', 'RLHF\\n(3+ models)']\n",
        "memory = [1, 2, 3.5]\n",
        "colors = ['#4CAF50', '#2196F3', '#FF5722']\n",
        "bars = axes[2].bar(methods, memory, color=colors, edgecolor='black')\n",
        "axes[2].set_ylabel('Memory Usage (‡πÄ‡∏ó‡πà‡∏≤)', fontsize=10)\n",
        "axes[2].set_title('‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Memory', fontsize=12)\n",
        "for bar, mem in zip(bars, memory):\n",
        "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "                 f'{mem}x', ha='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('dpo_visualization.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:\")\n",
        "print(\"- ‡∏Å‡∏£‡∏≤‡∏ü 1: Sigmoid ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 0-1\")\n",
        "print(\"- ‡∏Å‡∏£‡∏≤‡∏ü 2: ‡∏¢‡∏¥‡πà‡∏á Thai > Eng ‡∏°‡∏≤‡∏Å ‡∏¢‡∏¥‡πà‡∏á loss ‡∏ï‡πà‡∏≥ (‡∏î‡∏µ)\")\n",
        "print(\"- ‡∏Å‡∏£‡∏≤‡∏ü 3: DPO ‡πÉ‡∏ä‡πâ memory 2 ‡πÄ‡∏ó‡πà‡∏≤ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î 2 ‡πÇ‡∏°‡πÄ‡∏î‡∏•\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xTtZLX_mzNs"
      },
      "outputs": [],
      "source": [
        "def compute_log_probs(logits, labels, mask=None):\n",
        "    \"\"\"\n",
        "    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì log probability ‡∏Ç‡∏≠‡∏á tokens\n",
        "    \"\"\"\n",
        "    # Shift: logits[:-1] ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ labels[1:]\n",
        "    logits = logits[:, :-1, :]\n",
        "    labels = labels[:, 1:]\n",
        "\n",
        "    # Log softmax\n",
        "    log_softmax = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "    # Gather log probs\n",
        "    selected_log_probs = torch.gather(\n",
        "        log_softmax,\n",
        "        dim=-1,\n",
        "        index=labels.unsqueeze(-1)\n",
        "    ).squeeze(-1)\n",
        "\n",
        "    if mask is not None:\n",
        "        mask = mask[:, 1:]\n",
        "        selected_log_probs = selected_log_probs * mask\n",
        "        avg_log_prob = selected_log_probs.sum(-1) / mask.sum(-1).clamp(min=1)\n",
        "    else:\n",
        "        avg_log_prob = selected_log_probs.mean(-1)\n",
        "\n",
        "    return avg_log_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_5I1bkXmzNs"
      },
      "outputs": [],
      "source": [
        "def compute_dpo_loss(policy_model, reference_model, batch, beta=0.1):\n",
        "    \"\"\"\n",
        "    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì DPO Loss\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    policy_model: Qwen3Model - ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞ train\n",
        "    reference_model: Qwen3Model - ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á (frozen)\n",
        "    batch: dict - Batch ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "    beta: float - Temperature parameter\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    loss, chosen_reward, rejected_reward\n",
        "    \"\"\"\n",
        "    chosen = batch[\"chosen\"]\n",
        "    rejected = batch[\"rejected\"]\n",
        "    chosen_mask = batch[\"chosen_mask\"]\n",
        "    rejected_mask = batch[\"rejected_mask\"]\n",
        "\n",
        "    # Forward pass - Policy model\n",
        "    policy_chosen_logits = policy_model(chosen)\n",
        "    policy_rejected_logits = policy_model(rejected)\n",
        "\n",
        "    # Forward pass - Reference model (no gradient)\n",
        "    with torch.no_grad():\n",
        "        ref_chosen_logits = reference_model(chosen)\n",
        "        ref_rejected_logits = reference_model(rejected)\n",
        "\n",
        "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì log probs\n",
        "    policy_chosen_log_probs = compute_log_probs(policy_chosen_logits, chosen, chosen_mask)\n",
        "    policy_rejected_log_probs = compute_log_probs(policy_rejected_logits, rejected, rejected_mask)\n",
        "    ref_chosen_log_probs = compute_log_probs(ref_chosen_logits, chosen, chosen_mask)\n",
        "    ref_rejected_log_probs = compute_log_probs(ref_rejected_logits, rejected, rejected_mask)\n",
        "\n",
        "    # Log ratios\n",
        "    chosen_log_ratio = policy_chosen_log_probs - ref_chosen_log_probs\n",
        "    rejected_log_ratio = policy_rejected_log_probs - ref_rejected_log_probs\n",
        "\n",
        "    # DPO Loss\n",
        "    logits = beta * (chosen_log_ratio - rejected_log_ratio)\n",
        "    loss = -F.logsigmoid(logits).mean()\n",
        "\n",
        "    # Rewards for monitoring\n",
        "    chosen_reward = chosen_log_ratio.mean().detach()\n",
        "    rejected_reward = rejected_log_ratio.mean().detach()\n",
        "\n",
        "    return loss, chosen_reward, rejected_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvjkY__hmzNs"
      },
      "source": [
        "---\n",
        "## 7. ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ DPO\n",
        "\n",
        "### 7.1 ‡πÇ‡∏´‡∏•‡∏î Qwen3-0.6B Pre-trained Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWZFWJHImzNs"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Qwen3-0.6B weights...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Download weights\n",
        "weights = download_from_huggingface_from_snapshots(\n",
        "    repo_id=REPO_ID,\n",
        "    local_dir=LOCAL_DIR\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Downloaded {len(weights)} weight tensors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m94Y71AmmzNs"
      },
      "outputs": [],
      "source": [
        "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Policy Model...\")\n",
        "policy_model = Qwen3Model(QWEN_CONFIG_06_B)\n",
        "load_weights_into_qwen3(policy_model, weights, QWEN_CONFIG_06_B)\n",
        "policy_model.to(device)\n",
        "\n",
        "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Reference Model...\")\n",
        "reference_model = Qwen3Model(QWEN_CONFIG_06_B)\n",
        "load_weights_into_qwen3(reference_model, weights, QWEN_CONFIG_06_B)\n",
        "reference_model.to(device)\n",
        "\n",
        "# Freeze reference model\n",
        "for param in reference_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in policy_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in policy_model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à!\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"\\n‚ö†Ô∏è ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÉ‡∏ä‡πâ memory ~{total_params * 4 / 1e9:.1f} GB (2 ‡πÇ‡∏°‡πÄ‡∏î‡∏• √ó BF16)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2r1K-sImzNs"
      },
      "source": [
        "### 7.2 Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4iXJuPumzNs"
      },
      "outputs": [],
      "source": [
        "def train_dpo(\n",
        "    policy_model,\n",
        "    reference_model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    num_epochs=3,\n",
        "    beta=0.1,\n",
        "    eval_freq=10\n",
        "):\n",
        "    \"\"\"\n",
        "    Training loop ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DPO\n",
        "    \"\"\"\n",
        "    train_losses, val_losses, reward_margins = [], [], []\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        policy_model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss, chosen_reward, rejected_reward = compute_dpo_loss(\n",
        "                policy_model, reference_model, batch, beta=beta\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(policy_model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            reward_margin = chosen_reward.item() - rejected_reward.item()\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'margin': f'{reward_margin:.4f}'\n",
        "            })\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "            if global_step % eval_freq == 0:\n",
        "                val_loss = evaluate_dpo(policy_model, reference_model, val_loader, beta)\n",
        "                train_losses.append(loss.item())\n",
        "                val_losses.append(val_loss)\n",
        "                reward_margins.append(reward_margin)\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        print(f\"\\nEpoch {epoch+1}: Average Loss = {avg_loss:.4f}\")\n",
        "\n",
        "    return {'train_losses': train_losses, 'val_losses': val_losses, 'reward_margins': reward_margins}\n",
        "\n",
        "\n",
        "def evaluate_dpo(policy_model, reference_model, val_loader, beta=0.1):\n",
        "    policy_model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            loss, _, _ = compute_dpo_loss(policy_model, reference_model, batch, beta=beta)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    policy_model.train()\n",
        "    return total_loss / len(val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_VApBGlmzNs"
      },
      "outputs": [],
      "source": [
        "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    policy_model.parameters(),\n",
        "    lr=5e-6,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "print(\"Optimizer: AdamW\")\n",
        "print(\"Learning Rate: 5e-6\")\n",
        "print(\"Beta (DPO): 0.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD9PgTGJmzNs"
      },
      "outputs": [],
      "source": [
        "# ‡πÄ‡∏£‡∏¥‡πà‡∏° Training\n",
        "print(\"=\"*60)\n",
        "print(\"‡πÄ‡∏£‡∏¥‡πà‡∏° DPO Training with Qwen3-0.6B\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "history = train_dpo(\n",
        "    policy_model=policy_model,\n",
        "    reference_model=reference_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=2,\n",
        "    beta=0.1,\n",
        "    eval_freq=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_mThVGGmzNt"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].plot(history['train_losses'], label='Train Loss', color='blue')\n",
        "axes[0].plot(history['val_losses'], label='Val Loss', color='orange')\n",
        "axes[0].set_xlabel('Evaluation Steps')\n",
        "axes[0].set_ylabel('DPO Loss')\n",
        "axes[0].set_title('Training & Validation Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(history['reward_margins'], color='green')\n",
        "axes[1].set_xlabel('Evaluation Steps')\n",
        "axes[1].set_ylabel('Reward Margin (Thai - Eng)')\n",
        "axes[1].set_title('Chosen - Rejected Reward Margin')\n",
        "axes[1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('dpo_training_curves.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä ‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á:\")\n",
        "print(\"- Loss ‡∏Ñ‡∏ß‡∏£‡∏•‡∏î‡∏•‡∏á\")\n",
        "print(\"- Reward Margin ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô (Thai > Eng)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t2Q3T4mmzNt"
      },
      "source": [
        "---\n",
        "## 8. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqqrdQFkmzNt"
      },
      "outputs": [],
      "source": [
        "def generate_response(model, tokenizer, prompt, max_new_tokens=200, temperature=0.6):\n",
        "    \"\"\"\n",
        "    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å Qwen3\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Format prompt\n",
        "    formatted = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "    input_ids = tokenizer.encode(formatted, chat_wrapped=False)\n",
        "    input_tensor = torch.tensor([input_ids]).to(device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits = model(input_tensor)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            input_tensor = torch.cat([input_tensor, next_token], dim=1)\n",
        "\n",
        "            # Stop at EOS\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    output = tokenizer.decode(input_tensor[0].tolist())\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIqRyL7RmzNt"
      },
      "outputs": [],
      "source": [
        "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
        "test_prompts = [\n",
        "    \"‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ß‡πà‡∏≤ Machine Learning ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£\",\n",
        "    \"‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏î‡∏µ\",\n",
        "    \"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏°‡∏µ‡∏Å‡∏µ‡πà‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î\"\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"‡∏ó‡∏î‡∏™‡∏≠‡∏ö Generation ‡∏´‡∏•‡∏±‡∏á DPO Training\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\nüìù ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° {i+1}: {prompt}\")\n",
        "    print(\"-\"*40)\n",
        "    response = generate_response(policy_model, tokenizer, prompt)\n",
        "    print(response)\n",
        "    print(\"-\"*40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQJNH1t3mzNw"
      },
      "source": [
        "---\n",
        "## ‡∏™‡∏£‡∏∏‡∏õ\n",
        "\n",
        "### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
        "\n",
        "1. **DPO ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£**: ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÅ‡∏ô‡∏ß LLM ‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå\n",
        "\n",
        "2. **Qwen3-0.6B**: ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞ Thinking Mode\n",
        "\n",
        "3. **Memory Requirements**: DPO ‡πÉ‡∏ä‡πâ memory 2 ‡πÄ‡∏ó‡πà‡∏≤‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î 2 ‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
        "\n",
        "4. **DPO Loss**: ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ä‡∏≠‡∏ö chosen (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢) ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ rejected (‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)\n",
        "\n",
        "### Use Case ‡πÉ‡∏ô‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ô‡∏µ‡πâ:\n",
        "\n",
        "- **Chosen**: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ + `<think>` reasoning\n",
        "- **Rejected**: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© (‡∏ú‡∏¥‡∏î‡∏†‡∏≤‡∏©‡∏≤)\n",
        "- **‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå**: ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ñ‡∏π‡∏Å‡∏ñ‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
        "\n",
        "---\n",
        "\n",
        "**‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:**\n",
        "\n",
        "- Raschka, Sebastian. *Build A Large Language Model (From Scratch)*. Manning, 2024. ISBN: 978-1633437166.\n",
        "- Rafailov et al. \"Direct Preference Optimization: Your Language Model is Secretly a Reward Model\" NeurIPS 2023.\n",
        "- https://github.com/rasbt/LLMs-from-scratch\n",
        "- https://huggingface.co/Qwen/Qwen3-0.6B\n",
        "\n",
        "---\n",
        "\n",
        "**‡πÅ‡∏õ‡∏•‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÇ‡∏î‡∏¢:** Kobkrit Viriyayudhakorn (‡∏Å‡∏≠‡∏ö‡∏Å‡∏§‡∏ï‡∏¢‡πå ‡∏ß‡∏¥‡∏£‡∏¥‡∏¢‡∏∞‡∏¢‡∏∏‡∏ó‡∏ò‡∏Å‡∏£) | kobkrit@iapp.co.th | iApp Technology Co., Ltd."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbVYcfLHmzNw"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "<b>‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô DPO (Direct Preference Optimization) ‡∏à‡∏≤‡∏Å‡∏®‡∏π‡∏ô‡∏¢‡πå</b><br>\n",
        "‡πÅ‡∏õ‡∏•‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠ <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> ‡πÇ‡∏î‡∏¢ <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
        "<br>Repository ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "---\n",
        "\n",
        "**‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:**\n",
        "> Raschka, Sebastian. *Build A Large Language Model (From Scratch)*. Manning, 2024. ISBN: 978-1633437166.\n",
        "\n",
        "**‡πÅ‡∏õ‡∏•‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÇ‡∏î‡∏¢:**\n",
        "> **Kobkrit Viriyayudhakorn (‡∏Å‡∏≠‡∏ö‡∏Å‡∏§‡∏ï‡∏¢‡πå ‡∏ß‡∏¥‡∏£‡∏¥‡∏¢‡∏∞‡∏¢‡∏∏‡∏ó‡∏ò‡∏Å‡∏£)**<br>\n",
        "> Email: kobkrit@iapp.co.th<br>\n",
        "> iApp Technology Co., Ltd.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}