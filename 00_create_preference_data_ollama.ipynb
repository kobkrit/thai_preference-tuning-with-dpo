{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Preference Dataset\n",
    "\n",
    "## ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç\n",
    "1. [‡∏ö‡∏ó‡∏ô‡∏≥](#1-‡∏ö‡∏ó‡∏ô‡∏≥)\n",
    "2. [‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Preference](#2-‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•-preference)\n",
    "3. [‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Ollama](#3-‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á-ollama)\n",
    "4. [‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ LLM](#4-‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢-llm)\n",
    "5. [‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢](#5-‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. ‡∏ö‡∏ó‡∏ô‡∏≥\n",
    "\n",
    "**Preference Finetuning** ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÅ‡∏ô‡∏ß LLM ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£ Instruction Finetuning ‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "\n",
    "### ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "\n",
    "‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DPO ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"instruction\": \"‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\",\n",
    "    \"input\": \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° (optional)\",\n",
    "    \"chosen\": \"‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏î‡∏µ (‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)\",\n",
    "    \"rejected\": \"‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏î‡∏µ (‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)\"\n",
    "}\n",
    "```\n",
    "\n",
    "### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"instruction\": \"‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢\",\n",
    "    \"input\": \"\",\n",
    "    \"chosen\": \"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏ï‡∏±‡πâ‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏≠‡πÄ‡∏ä‡∏µ‡∏¢‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡πÄ‡∏â‡∏µ‡∏¢‡∏á‡πÉ‡∏ï‡πâ ‡∏°‡∏µ‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏Ø ‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏≠‡∏±‡∏ô‡πÄ‡∏Å‡πà‡∏≤‡πÅ‡∏Å‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡πÉ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏≠‡∏£‡πà‡∏≠‡∏¢\",\n",
    "    \"rejected\": \"‡πÑ‡∏ó‡∏¢‡∏Å‡πá‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏ñ‡∏ß‡πÜ ‡πÄ‡∏≠‡πÄ‡∏ä‡∏µ‡∏¢ ‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á ‡∏Å‡πá‡πÅ‡∏Ñ‡πà‡∏ô‡∏±‡πâ‡∏ô\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Preference\n",
    "\n",
    "‡∏°‡∏µ 3 ‡∏ß‡∏¥‡∏ò‡∏µ‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:\n",
    "\n",
    "### ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡πÉ‡∏´‡πâ‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö\n",
    "- ‡πÉ‡∏ä‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡πÅ‡∏ö‡∏ö\n",
    "- ‡πÉ‡∏´‡πâ‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ß‡πà‡∏≤‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤\n",
    "- **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ**: ‡πÑ‡∏î‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á\n",
    "- **‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢**: ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏™‡∏π‡∏á\n",
    "\n",
    "### ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡πÉ‡∏ä‡πâ LLM ‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö\n",
    "- ‡πÉ‡∏ä‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡πÅ‡∏ö‡∏ö\n",
    "- ‡πÉ‡∏ä‡πâ LLM ‡∏≠‡∏µ‡∏Å‡∏ï‡∏±‡∏ß (‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô) ‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö\n",
    "- **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ**: ‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ñ‡∏π‡∏Å‡∏Å‡∏ß‡πà‡∏≤\n",
    "- **‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢**: ‡∏≠‡∏≤‡∏à‡∏°‡∏µ bias\n",
    "\n",
    "### ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 3: ‡πÉ‡∏ä‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏π‡πà‡∏ï‡∏£‡∏á‡πÜ ‚¨ÖÔ∏è (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô notebook ‡∏ô‡∏µ‡πâ)\n",
    "- ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÉ‡∏´‡πâ LLM (‡πÄ‡∏ä‡πà‡∏ô ‡∏™‡∏∏‡∏†‡∏≤‡∏û vs ‡πÑ‡∏°‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û)\n",
    "- ‡πÉ‡∏´‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏±‡πâ‡∏á chosen ‡πÅ‡∏•‡∏∞ rejected\n",
    "- **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ**: ‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "- **‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢**: ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Ollama\n",
    "\n",
    "**Ollama** ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ô LLM ‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤\n",
    "\n",
    "### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á:\n",
    "\n",
    "1. ‡πÑ‡∏õ‡∏ó‡∏µ‡πà https://ollama.com\n",
    "2. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
    "3. ‡πÄ‡∏õ‡∏¥‡∏î Terminal ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô:\n",
    "\n",
    "```bash\n",
    "# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Llama 3.1 (8B)\n",
    "ollama pull llama3.1\n",
    "\n",
    "# ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡∏°‡∏µ RAM ‡πÄ‡∏¢‡∏≠‡∏∞ ‡πÉ‡∏ä‡πâ 70B\n",
    "ollama pull llama3.1:70b\n",
    "```\n",
    "\n",
    "4. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö:\n",
    "\n",
    "```bash\n",
    "ollama run llama3.1\n",
    ">>> ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ollama(prompt, model=\"llama3.1\", url=\"http://localhost:11434/api/chat\"):\n",
    "    \"\"\"\n",
    "    ‡∏™‡πà‡∏á prompt ‡πÑ‡∏õ‡∏´‡∏≤ Ollama ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prompt: str\n",
    "        ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á‡πÑ‡∏õ\n",
    "    model: str\n",
    "        ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏• (default: llama3.1)\n",
    "    url: str\n",
    "        URL ‡∏Ç‡∏≠‡∏á Ollama API\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"options\": {\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0.7,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with requests.post(url, json=data, stream=True, timeout=60) as r:\n",
    "            r.raise_for_status()\n",
    "            response_data = \"\"\n",
    "            for line in r.iter_lines(decode_unicode=True):\n",
    "                if not line:\n",
    "                    continue\n",
    "                response_json = json.loads(line)\n",
    "                if \"message\" in response_json:\n",
    "                    response_data += response_json[\"message\"][\"content\"]\n",
    "        return response_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama\n",
    "# ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏±‡∏ô 'ollama serve' ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ô terminal ‡∏≠‡∏∑‡πà‡∏ô\n",
    "\n",
    "test_response = query_ollama(\"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡πà‡∏≠‡∏¢\")\n",
    "if test_response:\n",
    "    print(\"‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n",
    "    print(f\"\\n‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\\n{test_response}\")\n",
    "else:\n",
    "    print(\"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama ‡πÑ‡∏î‡πâ\")\n",
    "    print(\"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤:\")\n",
    "    print(\"1. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Ollama ‡πÅ‡∏•‡πâ‡∏ß\")\n",
    "    print(\"2. ‡∏£‡∏±‡∏ô 'ollama serve' ‡πÉ‡∏ô terminal\")\n",
    "    print(\"3. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ 'ollama pull llama3.1'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ LLM\n",
    "\n",
    "‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏π‡πà‡∏Ç‡∏≠‡∏á chosen/rejected responses\n",
    "\n",
    "### ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:\n",
    "- **Chosen**: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå\n",
    "- **Rejected**: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡πÑ‡∏°‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    \"\"\"\n",
    "    ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö prompt\n",
    "    \"\"\"\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    \n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry.get(\"input\") else \"\"\n",
    "    \n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_preference_pair(entry, model=\"llama3.1\"):\n",
    "    \"\"\"\n",
    "    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏π‡πà chosen/rejected ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö entry ‡∏´‡∏ô‡∏∂‡πà‡∏á\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    entry: dict\n",
    "        ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ instruction ‡πÅ‡∏•‡∏∞ output\n",
    "    model: str\n",
    "        ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: entry ‡∏ó‡∏µ‡πà‡∏°‡∏µ chosen ‡πÅ‡∏•‡∏∞ rejected ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n",
    "    \"\"\"\n",
    "    formatted_input = format_input(entry)\n",
    "    original_output = entry.get(\"output\", \"\")\n",
    "    \n",
    "    # ‡∏™‡∏∏‡πà‡∏°‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡∏†‡∏≤‡∏û‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û\n",
    "    politeness = random.choice([\"polite\", \"impolite\"])\n",
    "    \n",
    "    prompt = f\"\"\"Given the following instruction and original response, rewrite the response to be more {politeness}.\n",
    "\n",
    "Instruction: {entry['instruction']}\n",
    "Original Response: {original_output}\n",
    "\n",
    "Rules:\n",
    "- If 'polite': Make it more helpful, detailed, and professional\n",
    "- If 'impolite': Make it short, dismissive, or slightly rude (but still correct)\n",
    "- Keep the core information the same\n",
    "- Only return the rewritten response, nothing else\n",
    "- Respond in the same language as the original\n",
    "\n",
    "Rewritten response:\"\"\"\n",
    "    \n",
    "    rewritten = query_ollama(prompt, model=model)\n",
    "    \n",
    "    if rewritten:\n",
    "        if politeness == \"polite\":\n",
    "            entry[\"chosen\"] = rewritten.strip()\n",
    "            entry[\"rejected\"] = original_output\n",
    "        else:\n",
    "            entry[\"chosen\"] = original_output\n",
    "            entry[\"rejected\"] = rewritten.strip()\n",
    "    else:\n",
    "        # Fallback: ‡πÉ‡∏ä‡πâ original ‡πÄ‡∏õ‡πá‡∏ô chosen\n",
    "        entry[\"chosen\"] = original_output\n",
    "        entry[\"rejected\"] = original_output[:len(original_output)//2]  # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏£‡∏∂‡πà‡∏á\n",
    "    \n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
    "sample_entry = {\n",
    "    \"instruction\": \"‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ß‡πà‡∏≤ Python ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"Python ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏°‡∏¥‡πà‡∏á\"\n",
    "}\n",
    "\n",
    "print(\"‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á Preference:\")\n",
    "print(json.dumps(sample_entry, indent=2, ensure_ascii=False))\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á preference pair (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Ollama ‡∏£‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà)\n",
    "# result = generate_preference_pair(sample_entry)\n",
    "# print(\"\\n‡∏´‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Preference:\")\n",
    "# print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_preference_dataset(data, model=\"llama3.1\", max_entries=None):\n",
    "    \"\"\"\n",
    "    ‡∏™‡∏£‡πâ‡∏≤‡∏á preference dataset ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• instruction\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data: list\n",
    "        ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á entries\n",
    "    model: str\n",
    "        ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ\n",
    "    max_entries: int, optional\n",
    "        ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô entries ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list: preference dataset\n",
    "    \"\"\"\n",
    "    if max_entries:\n",
    "        data = data[:max_entries]\n",
    "    \n",
    "    preference_data = []\n",
    "    \n",
    "    for entry in tqdm(data, desc=\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\"):\n",
    "        result = generate_preference_pair(entry.copy(), model=model)\n",
    "        preference_data.append(result)\n",
    "    \n",
    "    return preference_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
    "\n",
    "‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Preference ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\n",
    "with open(\"dpo_thai_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    thai_dpo_data = json.load(f)\n",
    "\n",
    "print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {len(thai_dpo_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\n",
    "for i in range(min(3, len(thai_dpo_data))):\n",
    "    entry = thai_dpo_data[i]\n",
    "    print(\"=\"*60)\n",
    "    print(f\"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà {i+1}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nüìù Instruction: {entry['instruction']}\")\n",
    "    if entry.get('input'):\n",
    "        print(f\"üì• Input: {entry['input']}\")\n",
    "    print(f\"\\n‚úÖ Chosen (‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏î‡∏µ):\")\n",
    "    print(f\"{entry['chosen']}\")\n",
    "    print(f\"\\n‚ùå Rejected (‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏î‡∏µ):\")\n",
    "    print(f\"{entry['rejected']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô JSON\n",
    "def save_preference_data(data, filename):\n",
    "    \"\"\"\n",
    "    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏ó‡∏µ‡πà: {filename}\")\n",
    "\n",
    "# save_preference_data(preference_data, \"my_preference_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‡∏™‡∏£‡∏∏‡∏õ\n",
    "\n",
    "### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
    "\n",
    "1. **‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Preference**: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ instruction, chosen, rejected\n",
    "\n",
    "2. **‡∏ß‡∏¥‡∏ò‡∏µ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**: \n",
    "   - ‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö\n",
    "   - LLM ‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö\n",
    "   - LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏π‡πà‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á\n",
    "\n",
    "3. **‡πÉ‡∏ä‡πâ Ollama**: ‡∏£‡∏±‡∏ô LLM ‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "\n",
    "4. **‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏≠‡∏ö**: ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö ‡πÄ‡∏ä‡πà‡∏ô:\n",
    "   - ‡∏™‡∏∏‡∏†‡∏≤‡∏û vs ‡πÑ‡∏°‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û\n",
    "   - ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î vs ‡∏™‡∏±‡πâ‡∏ô\n",
    "   - ‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå vs ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå\n",
    "\n",
    "---\n",
    "\n",
    "**‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ**: ‡πÑ‡∏õ‡∏¢‡∏±‡∏á `01_dpo_from_scratch_thai.ipynb` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ DPO\n",
    "\n",
    "---\n",
    "\n",
    "**‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:**\n",
    "\n",
    "- Raschka, Sebastian. *Build A Large Language Model (From Scratch)*. Manning, 2024. ISBN: 978-1633437166.\n",
    "- https://github.com/rasbt/LLMs-from-scratch\n",
    "- https://ollama.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "<b>‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Preference ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DPO</b><br>\n",
    "‡πÅ‡∏õ‡∏•‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠ <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> ‡πÇ‡∏î‡∏¢ <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Repository ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "**‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:**\n",
    "> Raschka, Sebastian. *Build A Large Language Model (From Scratch)*. Manning, 2024. ISBN: 978-1633437166.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
